# Vector Translation API

API Flask Ä‘á»ƒ trÃ­ch xuáº¥t vector tá»« hÃ¬nh áº£nh sá»­ dá»¥ng CLIP vÃ  ResNet-50 models vá»›i há»— trá»£ GPU.

## ğŸš€ TÃ­nh nÄƒng

### âœ… **GPU Support**
- Tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  sá»­ dá»¥ng GPU (CUDA) náº¿u cÃ³ sáºµn
- Fallback sang CPU náº¿u GPU khÃ´ng kháº£ dá»¥ng
- Optimized memory management vÃ  GPU utilization

### âœ… **Models há»— trá»£**
- **CLIP Model**: `patrickjohncyh/fashion-clip` - Vector embeddings cho image + text
- **ResNet-50**: Feature extraction tá»« hÃ¬nh áº£nh

### âœ… **API Endpoints**
- `POST /get_clip_vector` - TrÃ­ch xuáº¥t CLIP vector
- `POST /get_resnet_vector` - TrÃ­ch xuáº¥t ResNet-50 vector  
- `GET /health` - Health check vÃ  resource monitoring

### âœ… **Fault Tolerance**
- Comprehensive error handling
- Memory monitoring vÃ  automatic cleanup
- Health check endpoints
- Graceful degradation

## ğŸ“‹ Requirements

### GPU Environment
```bash
# NVIDIA Docker runtime
nvidia-docker2
# hoáº·c Docker vá»›i nvidia-container-toolkit

# CUDA 12.1+ compatible GPU
# Minimum 4GB GPU memory recommended
```

### CPU Fallback
```bash
# Standard Docker installation
docker >= 20.10
docker-compose >= 1.29
```

## ğŸ›  Installation & Usage

### 1. Kiá»ƒm tra GPU Support
```bash
# Kiá»ƒm tra GPU
nvidia-smi

# Kiá»ƒm tra NVIDIA Docker
docker run --rm --gpus all nvidia/cuda:12.1-runtime-ubuntu22.04 nvidia-smi
```

### 2. Build vÃ  Run vá»›i GPU
```bash
# Clone repository
git clone <repo-url>
cd vector-translation

# Build vÃ  run vá»›i GPU
docker-compose up --build

# Hoáº·c chá»‰ GPU service
docker-compose up vector-translation-gpu
```

### 3. CPU Fallback
```bash
# Náº¿u khÃ´ng cÃ³ GPU, sá»­ dá»¥ng CPU version
docker-compose --profile cpu-fallback up vector-translation-cpu
```

### 4. Local Development
```bash
# Install dependencies
pip install -r requirements.txt

# Download models (náº¿u cáº§n)
python download_model.py

# Run local
python clip_api_local.py
```

## ğŸ“Š API Usage

### CLIP Vector Extraction
```bash
# Vá»›i file upload
curl -X POST http://localhost:5000/get_clip_vector \
  -F "image_file=@sample.jpg" \
  -F "text=fashion clothing"

# Vá»›i URL
curl -X POST http://localhost:5000/get_clip_vector \
  -F "image_url=https://example.com/image.jpg" \
  -F "text=fashion clothing"
```

### ResNet-50 Vector Extraction  
```bash
# Vá»›i file upload
curl -X POST http://localhost:5000/get_resnet_vector \
  -F "image_file=@sample.jpg"

# Vá»›i URL
curl -X POST http://localhost:5000/get_resnet_vector \
  -F "image_url=https://example.com/image.jpg"
```

### Health Check & Monitoring
```bash
# Health check
curl http://localhost:5000/health

# Response example:
{
  "status": "healthy",
  "models": {
    "clip_model_loaded": true,
    "resnet_model_loaded": true,
    "device": "cuda:0"
  },
  "resources": {
    "cpu_percent": 15.2,
    "memory_percent": 45.8,
    "memory_available_gb": 8.2,
    "memory_used_gb": 7.8,
    "gpu_memory_used_gb": 2.1,
    "gpu_memory_total_gb": 8.0,
    "gpu_memory_percent": 26.25,
    "gpu_utilization_percent": 12
  },
  "warnings": []
}
```

## ğŸ”§ Configuration

### Environment Variables
```bash
PORT=5000                    # API port
CUDA_VISIBLE_DEVICES=0       # GPU device selection
```

### Docker Compose Override
```yaml
# docker-compose.override.yml
version: '3.8'
services:
  vector-translation-gpu:
    environment:
      - CUDA_VISIBLE_DEVICES=0,1  # Multiple GPUs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2  # Use 2 GPUs
              capabilities: [gpu]
```

## ğŸ“ˆ Performance Monitoring

### GPU Utilization
```bash
# Real-time GPU monitoring
watch -n 1 nvidia-smi

# Container stats
docker stats vector-translation-gpu
```

### Memory Management
- Automatic memory cleanup after each request
- GPU memory cache clearing
- Memory usage warnings in health check

## ğŸš¨ Troubleshooting

### Common Issues

1. **GPU not detected**
   ```bash
   # Check NVIDIA drivers
   nvidia-smi
   
   # Check Docker GPU support
   docker run --rm --gpus all nvidia/cuda:12.1-runtime-ubuntu22.04 nvidia-smi
   ```

2. **Out of Memory (OOM)**
   ```bash
   # Reduce batch size hoáº·c image resolution
   # Monitor memory usage via /health endpoint
   curl http://localhost:5000/health
   ```

3. **Model loading errors**
   ```bash
   # Check logs
   docker logs vector-translation-gpu
   
   # Ensure internet connection for model download
   # Check disk space for model cache
   ```

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â–¶â”‚   Flask API      â”‚â”€â”€â”€â–¶â”‚   GPU/CPU       â”‚
â”‚                 â”‚    â”‚   (clip_api.py)  â”‚    â”‚   Processing    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Health Check   â”‚
                       â”‚   & Monitoring   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”’ Security Considerations

- Input validation cho image uploads
- File size limits
- Memory leak prevention
- Resource monitoring vÃ  alerts

## ğŸ“š Development

### Adding new models
1. Update model loading section
2. Add new endpoint
3. Update health check
4. Test GPU compatibility

### Testing
```bash
# Unit tests
python -m pytest tests/

# Load testing
ab -n 100 -c 10 http://localhost:5000/health
``` 